{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AioTaskQueue","text":"<p>AioTaskQueue is a modular task queue similar to  celery, arq, taskiq and alike.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#fully-typed","title":"Fully Typed","text":"<p>Tasks created with @task decorator are fully typed,  so you could catch errors such as passing wrong arguments to tasks early.</p>"},{"location":"#modularity","title":"Modularity","text":"<p>All of the components are loosely coupled, you're free to mix and match them however you want.</p>"},{"location":"#serialization-backends","title":"Serialization Backends","text":"<p>Using different serialization backends you can serialize models from pydantic,  msgspec and integrate any other library the same way.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0141-2025-10-20","title":"0.14.1 (2025-10-20)","text":""},{"location":"changelog/#0140-2025-10-15","title":"0.14.0 (2025-10-15)","text":""},{"location":"changelog/#feat","title":"Feat","text":"<ul> <li>redis: acquire lock when trying to perform maintenance tasks</li> <li>scheduled task broker</li> <li>sqlalchemy broker &amp; result backenC</li> </ul>"},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>move serialize_task call into Publisher.enqueue</li> </ul>"},{"location":"changelog/#0131-2025-05-23","title":"0.13.1 (2025-05-23)","text":""},{"location":"changelog/#fix","title":"Fix","text":"<ul> <li>convert TimeLimit into a dataclass</li> </ul>"},{"location":"changelog/#0130-2025-05-23","title":"0.13.0 (2025-05-23)","text":""},{"location":"changelog/#feat_1","title":"Feat","text":"<ul> <li>add built-in TimeLimitExtension</li> </ul>"},{"location":"changelog/#0120-2025-05-14","title":"0.12.0 (2025-05-14)","text":""},{"location":"changelog/#refactor_1","title":"Refactor","text":"<ul> <li>internal task management</li> </ul>"},{"location":"changelog/#0110-2025-05-12","title":"0.11.0 (2025-05-12)","text":""},{"location":"changelog/#feat_2","title":"Feat","text":"<ul> <li>add state to ExecutionContext</li> </ul>"},{"location":"changelog/#0100-2025-04-09","title":"0.10.0 (2025-04-09)","text":""},{"location":"changelog/#feat_3","title":"Feat","text":"<ul> <li>add OnTaskExecution extension (aka middleware)</li> </ul>"},{"location":"changelog/#090-2025-03-27","title":"0.9.0 (2025-03-27)","text":""},{"location":"changelog/#feat_4","title":"Feat","text":"<ul> <li>redis: automatically trim stream</li> <li>allow passing parameters directly into <code>@task</code> decorator</li> </ul>"},{"location":"changelog/#080-2025-03-24","title":"0.8.0 (2025-03-24)","text":""},{"location":"changelog/#feat_5","title":"Feat","text":"<ul> <li>add retry extension</li> </ul>"},{"location":"changelog/#070-2025-03-09","title":"0.7.0 (2025-03-09)","text":""},{"location":"changelog/#feat_6","title":"Feat","text":"<ul> <li>wrap ResultBackend.get return value into Some[] container type to differentiate between missing and None values</li> <li>add ResultBackend.get method</li> <li>allow enqueueing tasks with specific id</li> <li>result-backend: allow configuring result key in Configuration</li> </ul>"},{"location":"changelog/#fix_1","title":"Fix","text":"<ul> <li>skip tasks in \"sequential\" if there's a result present already</li> <li>scheduler: don't call OnScheduleExtension when initializing tasks on scheduler startup</li> </ul>"},{"location":"changelog/#060-2025-03-06","title":"0.6.0 (2025-03-06)","text":""},{"location":"changelog/#feat_7","title":"Feat","text":"<ul> <li>allow configuring result backend TTL</li> </ul>"},{"location":"changelog/#050-2025-03-06","title":"0.5.0 (2025-03-06)","text":""},{"location":"changelog/#feat_8","title":"Feat","text":"<ul> <li>worker: add shutdown deadline</li> </ul>"},{"location":"changelog/#040-2025-03-05","title":"0.4.0 (2025-03-05)","text":""},{"location":"changelog/#feat_9","title":"Feat","text":"<ul> <li>worker: add OnTaskException and OnTaskCompletion extensions</li> <li>scheduler: add OnScheduleExtension</li> </ul>"},{"location":"changelog/#030-2025-03-05","title":"0.3.0 (2025-03-05)","text":""},{"location":"changelog/#feat_10","title":"Feat","text":"<ul> <li>add built-in \"sequential\" task to run tasks in sequence</li> <li>allow injecting \"Publisher\" instances into tasks</li> </ul>"},{"location":"changelog/#020-2025-03-04","title":"0.2.0 (2025-03-04)","text":""},{"location":"changelog/#feat_11","title":"Feat","text":"<ul> <li>scheduler: allow passing TaskRouter instead of a list of tasks</li> <li>add redis result backend</li> <li>publisher: add default serialization backend to serialization backends by default</li> </ul>"},{"location":"changelog/#fix_2","title":"Fix","text":"<ul> <li>python 3.10-3.11 compatability</li> <li>redis: reclaim owned tasks in background</li> <li>change project name</li> </ul>"},{"location":"changelog/#refactor_2","title":"Refactor","text":"<ul> <li>rename projec to \"aiotaskqueue\"</li> </ul>"},{"location":"components/","title":"Overview","text":"<p>AioTaskQueue consists of multiple components that work together:</p>"},{"location":"components/#broker","title":"Broker","text":"<p>Broker communicates with the underlying queue and is needed to send and receive tasks.</p>"},{"location":"components/#publisher","title":"Publisher","text":"<p>Publisher is a common interface for queueing tasks to avoid duplicating logic between different brokers.</p>"},{"location":"components/#result-backend","title":"Result Backend","text":"<p>Result backend is needed to store execution results of task execution.</p>"},{"location":"components/#scheduler","title":"Scheduler","text":"<p>Scheduler is needed to schedule periodic tasks</p>"},{"location":"components/#worker","title":"Worker","text":"<p>Worker listens to a broker and runs tasks.</p>"},{"location":"components/publisher/","title":"Publisher","text":"<p>Publisher is a thin interface between your code and broker,  it's mostly responsible for serialization to not duplicate this logic over different brokers.</p>"},{"location":"components/publisher/#example-code","title":"Example Code:","text":"<pre><code>import asyncio\nimport logging\n\nfrom aiotaskqueue import Configuration, Publisher, task\nfrom aiotaskqueue.broker.inmemory import InMemoryBroker\nfrom aiotaskqueue.serialization.msgspec import MsgSpecSerializer\n\nlogger = logging.getLogger(\"app\")\n\n\n@task(name=\"send-email\")\nasync def send_email(email: str, message: str) -&gt; None:\n    logger.info(\"Sending email to %s, length: %s\", email, len(message))\n\n\nasync def main() -&gt; None:\n    configuration = Configuration(default_serialization_backend=MsgSpecSerializer())\n    broker = InMemoryBroker(max_buffer_size=100)\n\n    publisher = Publisher(broker=broker, config=configuration)\n    await publisher.enqueue(\n        send_email(email=\"email@example.com\", message=\"Hello, World!\")\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"components/result-backend/","title":"Result Backend","text":"<p>Result backend is used to store and retrieve execution results of your tasks.</p>"},{"location":"components/result-backend/#aiotaskqueue.result.abc.ResultBackend","title":"ResultBackend","text":""},{"location":"components/result-backend/#aiotaskqueue.result.abc.ResultBackend.set","title":"set  <code>async</code>","text":"<pre><code>set(task_id: str, value: TResult) -&gt; None\n</code></pre> <p>Set execution result.</p>"},{"location":"components/result-backend/#aiotaskqueue.result.abc.ResultBackend.get","title":"get  <code>async</code>","text":"<pre><code>get(\n    task_id: str,\n    definition: TaskDefinition[Any, TResult],\n) -&gt; Some[TResult] | None\n</code></pre> <p>Immediately try to retrieve execution result of task, returns Some(result) if result was stored, None otherwise.</p>"},{"location":"components/result-backend/#aiotaskqueue.result.abc.ResultBackend.wait","title":"wait  <code>async</code>","text":"<pre><code>wait(task: RunningTask[TResult]) -&gt; TResult\n</code></pre> <p>Wait on the task to finish and return the result.</p>"},{"location":"components/broker/","title":"Broker","text":""},{"location":"components/broker/#aiotaskqueue.broker.abc.Broker","title":"Broker","text":""},{"location":"components/broker/redis/","title":"Redis","text":"<p>Redis Streams</p> <p>For more info on Redis Streams please visit https://redis.io/docs/latest/develop/data-types/streams/</p>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBroker","title":"RedisBroker","text":"<pre><code>RedisBroker(\n    *,\n    redis: RedisClient,\n    broker_config: RedisBrokerConfig | None = None,\n    consumer_name: str,\n    max_concurrency: int = 20\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>redis</code>               (<code>RedisClient</code>)           \u2013            <p>Instance of redis</p> </li> <li> <code>broker_config</code>               (<code>RedisBrokerConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Redis specific configuration</p> </li> <li> <code>consumer_name</code>               (<code>str</code>)           \u2013            <p>Name of stream consumer, if you run multiple workers you'd need to change that. https://redis.io/docs/latest/develop/data-types/streams/#consumer-groups and https://redis.io/docs/latest/develop/data-types/streams/#differences-with-kafka-tm-partitions</p> </li> <li> <code>max_concurrency</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Max amount of tasks being concurrently added into redis stream</p> </li> </ul>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig","title":"RedisBrokerConfig  <code>dataclass</code>","text":"<pre><code>RedisBrokerConfig(\n    *,\n    stream_name: str = \"async-queue\",\n    maintenance_lock_name: str = \"aiotaskqueue-maintenance-lock\",\n    maintenance_lock_timeout: timedelta = timedelta(\n        minutes=10\n    ),\n    group_name: str = \"default\",\n    xread_block_time: timedelta = timedelta(seconds=1),\n    xread_count: int = 1,\n    xtrim_interval: timedelta = timedelta(minutes=30)\n)\n</code></pre>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig.stream_name","title":"stream_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stream_name: str = 'async-queue'\n</code></pre> <p>Stream name in redis (key name)</p>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig.group_name","title":"group_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_name: str = 'default'\n</code></pre> <p>Redis stream group name, there usually shouldn't be a need to change it. See https://redis.io/docs/latest/commands/xgroup-create/</p>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig.xread_block_time","title":"xread_block_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>xread_block_time: timedelta = timedelta(seconds=1)\n</code></pre> <p>BLOCK parameter passed to redis XREAD command</p>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig.xread_count","title":"xread_count  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>xread_count: int = 1\n</code></pre> <p>Amount of entries to read from stream at once</p>"},{"location":"components/broker/redis/#aiotaskqueue.broker.redis.RedisBrokerConfig.xtrim_interval","title":"xtrim_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>xtrim_interval: timedelta = timedelta(minutes=30)\n</code></pre> <p>Interval between XTRIM calls</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#aiotaskqueue.config.Configuration","title":"Configuration","text":"<pre><code>Configuration(\n    *,\n    task: TaskConfiguration | None = None,\n    result: ResultBackendConfiguration | None = None,\n    default_serialization_backend: SerializationBackend[\n        Any\n    ],\n    serialization_backends: Sequence[\n        SerializationBackend[Any]\n    ] = (),\n    extensions: Sequence[AnyExtension] = ()\n)\n</code></pre> <p>Configuration is a semi-global object that defines behavior shared between different components, such as serialization, plugins and timeouts.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>TaskConfiguration | None</code>, default:                   <code>None</code> )           \u2013            <p>task configuration</p> </li> <li> <code>default_serialization_backend</code>               (<code>SerializationBackend[Any]</code>)           \u2013            <p>default SerializationBackend</p> </li> <li> <code>serialization_backends</code>               (<code>Sequence[SerializationBackend[Any]]</code>, default:                   <code>()</code> )           \u2013            <p>list of serialization backends in order of priority</p> </li> </ul>"},{"location":"configuration/#aiotaskqueue.config.TaskConfiguration","title":"TaskConfiguration  <code>dataclass</code>","text":"<pre><code>TaskConfiguration(\n    healthcheck_interval: timedelta = timedelta(\n        seconds=5\n    ),\n    max_delivery_attempts: int = 3,\n    shutdown_deadline: timedelta = timedelta(\n        minutes=1\n    ),\n    timeout_interval: timedelta = timedelta(\n        seconds=10\n    ),\n)\n</code></pre>"},{"location":"configuration/#aiotaskqueue.config.TaskConfiguration.healthcheck_interval","title":"healthcheck_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>healthcheck_interval: timedelta = timedelta(seconds=5)\n</code></pre> <p>Interval in which worker should notify brokerthat task is being processed, if that's applicable.</p>"},{"location":"configuration/#aiotaskqueue.config.TaskConfiguration.timeout_interval","title":"timeout_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout_interval: timedelta = timedelta(seconds=10)\n</code></pre> <p>Interval in which task is considered stuck/failed.</p>"},{"location":"extensions/","title":"Extensions","text":"<p>Extensions are a way to extend aiotaskqueue, by handling certain events like task completion or errors, they're heavily inspired by StrawberryGraphQL Extensions.</p> <p>Multiple interfaces can be implemented by a single class to avoid having  to register multiple extensions for a single feature.</p>"},{"location":"extensions/#aiotaskqueue.extensions.OnTaskCompletion","title":"OnTaskCompletion","text":"<p>Called when task is successfully completed and the result is already stored in the ResultBackend.</p>"},{"location":"extensions/#aiotaskqueue.extensions.OnTaskException","title":"OnTaskException","text":"<p>Called when an exception was raised during task execution.</p>"},{"location":"extensions/#aiotaskqueue.extensions.OnTaskSchedule","title":"OnTaskSchedule","text":"<p>Called when task is scheduled and added to the queue.</p>"},{"location":"extensions/#aiotaskqueue.extensions.OnTaskExecution","title":"OnTaskExecution","text":"<p>Wraps task execution, working similarly to a middleware, it should return a value compatible with task return type.</p>"},{"location":"extensions/builtin/retry/","title":"Retry","text":"<p>Retry extension enqueues task again if it raised an exception. <pre><code>from aiotaskqueue import Configuration, task\nfrom aiotaskqueue.extensions.builtin import Retry, RetryExtension\nfrom aiotaskqueue.serialization.msgspec import MsgSpecSerializer\n\n\n@task(\n    name=\"name\",\n    markers=[Retry(max_retries=3)],  # (1)!\n)\nasync def some_task() -&gt; None:\n    pass\n\n\nconfiguration = Configuration(\n    default_serialization_backend=MsgSpecSerializer(),\n    extensions=[RetryExtension()],  # (2)!\n)\n</code></pre></p> <ol> <li>You need to add the <code>Retry</code> marker and configure amount of retries</li> <li>Don't forget to add the extension into your configuration</li> </ol> <p>Scheduling</p> <p>On retry task is simply added to the queue again, so the following applies:</p> <ul> <li>Task may be scheduled on different node</li> <li>Task won't be retried immediately if there are any tasks in front of it in the queue</li> </ul>"},{"location":"getting-started/declaring-tasks/","title":"Declaring Tasks","text":"<p>Tasks are what you'll be scheduling and what would be executed by the worker.</p> <p>To declare a task you simply need to decorate a function with <code>task</code> decorator:</p> <pre><code>from aiotaskqueue import task\n\n\n@task(name=\"task-name\")\nasync def my_task() -&gt; None:\n    pass\n</code></pre>"},{"location":"getting-started/declaring-tasks/#taskrouter","title":"TaskRouter","text":"<p>Task router is a simpler way to create a collection of tasks: <pre><code>from aiotaskqueue import TaskRouter\n\nrouter = TaskRouter()\n\n\n@router.task(name=\"task-name\")\nasync def my_task() -&gt; None:\n    pass\n</code></pre></p>"},{"location":"getting-started/declaring-tasks/#markers","title":"Markers","text":"<p>Different extensions could provide markers to add metadata to your task  definitions, for example to add retries to your tasks you could use <code>Retry</code> marker:</p> <pre><code>from aiotaskqueue import TaskRouter\nfrom aiotaskqueue.extensions.builtin import Retry\n\nrouter = TaskRouter()\n\n\n@router.task(\n    name=\"task-name\",\n    markers=[Retry(max_retries=3)],\n)\nasync def my_task() -&gt; None:\n    pass\n</code></pre> <p>Retry Extension</p> <p>In order for this specific example to work you need to add <code>RetryExtension</code> to your Configuration</p>"},{"location":"getting-started/publishing-tasks/","title":"Publishing Tasks","text":"<p>To publish your tasks you can use <code>Publisher</code> class that would be  associated with your <code>Configuration</code> and <code>Broker</code>: <pre><code>import asyncio\n\nfrom aiotaskqueue import Configuration, Publisher, task\nfrom aiotaskqueue.broker.inmemory import InMemoryBroker\nfrom aiotaskqueue.serialization.msgspec import MsgSpecSerializer\n\n\n@task(name=\"task\")\nasync def notify_user(user_id: int, message: str) -&gt; None:\n    pass\n\n\nasync def main() -&gt; None:\n    broker = InMemoryBroker(max_buffer_size=100)\n    configuration = Configuration(default_serialization_backend=MsgSpecSerializer())\n\n    publisher = Publisher(broker=broker, config=configuration)\n\n    await publisher.enqueue(\n        notify_user(\n            user_id=42,\n            message=\"Your notification!\",\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre></p>"},{"location":"getting-started/worker/","title":"Worker","text":"<p>Worker would read queue in your broker and execute all tasks it reads from it.</p> <p>To create a worker you need:</p> <ul> <li>Broker instance</li> <li>Configuration</li> <li>Task Router or list of your tasks.</li> <li>(Optionally) a result backend if you  want to store execution results of your tasks.</li> </ul> <pre><code>import asyncio\n\nfrom aiotaskqueue import Configuration, TaskRouter\nfrom aiotaskqueue.broker.redis import RedisBroker\nfrom aiotaskqueue.serialization.msgspec import MsgSpecSerializer\nfrom aiotaskqueue.worker import AsyncWorker\nfrom redis.asyncio import Redis\n\nrouter = TaskRouter()\n\n\n@router.task(\"my-task\")\nasync def my_task() -&gt; None:\n    pass\n\n\nasync def main() -&gt; None:\n    broker = RedisBroker(redis=Redis(), consumer_name=\"your-consumer-name\")\n    configuration = Configuration(default_serialization_backend=MsgSpecSerializer())\n\n    worker = AsyncWorker(\n        tasks=router,\n        broker=broker,\n        configuration=configuration,\n        concurrency=100,\n    )\n    await worker.run()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"internals/message-format/","title":"Message Format","text":"<p>All tasks are serialized by TaskRecord  msgspec model using json format</p> <p>Since parameters can be encoded using different serialization backends args, kwargs and result values are stored as a tuple (array in json) with serialization backend name and encoded value. <pre><code>{\n  \"id\":\"90ca5786-02c7-4320-a8d2-7db161f91955\",\n  \"task_name\":\"task-name\",\n  \"enqueue_time\":\"2025-03-27T09:59:25.185591Z\",\n  \"args\":[\n    [\"msgspec\",\"1\"],\n    [\"msgspec\",\"2\"],\n    [\"msgspec\",\"3\"]\n  ],\n  \"kwargs\": {\n    \"pydantic_arg\":[\"pydantic\",\"{\\\"a\\\":42,\\\"b\\\":\\\"str\\\"}\"],\n    \"msgspec_arg\":[\"msgspec\",\"{\\\"a\\\":42,\\\"b\\\":\\\"str\\\"}\"]\n  },\n  \"meta\":{\n    \"retry_count\": 1\n  }\n}\n</code></pre></p>"},{"location":"internals/message-format/#aiotaskqueue.serialization.TaskRecord","title":"TaskRecord","text":"Source code in <code>aiotaskqueue/serialization/_serialization.py</code> <pre><code>class TaskRecord(msgspec.Struct, kw_only=True):\n    id: str\n    task_name: str\n    requeue_count: int = 0\n    enqueue_time: datetime\n    args: tuple[tuple[SerializationBackendId, str], ...]\n    kwargs: dict[str, tuple[SerializationBackendId, str]]\n    meta: dict[str, Any] = msgspec.field(default_factory=dict)\n</code></pre>"}]}